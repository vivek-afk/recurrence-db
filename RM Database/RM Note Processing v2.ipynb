{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of notes: 72695\n"
     ]
    }
   ],
   "source": [
    "### load csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "file_name='HNDB_Progress_Notes_clean.csv'\n",
    "\n",
    "data=pd.read_csv(file_name,index_col='date_of_service_dttm',parse_dates=True,error_bad_lines=False)\n",
    "\n",
    "cols=['mrn','note_id','ip_note_type','author_service','note_narr']\n",
    "data=data[cols]\n",
    "data.dropna(subset=['mrn','note_narr'])\n",
    "\n",
    "### for 8980\n",
    "# file='all_mrns_of_interest.csv'\n",
    "# mrn_csv=pd.read_csv(file)\n",
    "# df_8980=mrn_csv[mrn_csv['Source']=='8980']\n",
    "# mrns_8980=df_8980.MRN.unique()\n",
    "# mrns_in_PNs=[]\n",
    "# mrns_not_in_PNs=[]\n",
    "# print ('Number of 8980 ptxs:',len(mrns_8980))\n",
    "# count=0\n",
    "# for mrn in mrns_8980:\n",
    "#     if mrn in data['mrn'].unique():\n",
    "#         mrns_in_PNs.append(mrn)\n",
    "#         count+=1\n",
    "#     else:\n",
    "#         mrns_not_in_PNs.append(mrn)\n",
    "# print ('number of 8980 ptxs in notes:',count)\n",
    "# print ('mrns with PNs:',mrns_in_PNs)\n",
    "# print ('mrns without PNs:',mrns_not_in_PNs)\n",
    "\n",
    "\n",
    "print ('Number of notes:',data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of notes from relevant services: 25593\n"
     ]
    }
   ],
   "source": [
    "### filter out for desired services\n",
    "#print (data.author_service.unique())\n",
    "\n",
    "#services=['HEMATOLOGY/ONCOLOGY','OTOLARYNGOLOGY','RADIATION ONCOLOGY']\n",
    "services=['HEMATOLOGY/ONCOLOGY']\n",
    "data=data[data['author_service'].isin(services)]\n",
    "print ('Number of notes from relevant services:',data.shape[0])\n",
    "\n",
    "### for 8980\n",
    "# file='all_mrns_of_interest.csv'\n",
    "# mrn_csv=pd.read_csv(file)\n",
    "# df_8980=mrn_csv[mrn_csv['Source']=='8980']\n",
    "# mrns_8980=df_8980.MRN.unique()\n",
    "# count=0\n",
    "# for mrn in mrns_8980:\n",
    "#     if mrn in data['mrn'].unique():\n",
    "#         count+=1\n",
    "# print ('number of 8980 ptxs in notes:',count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mrns are already in correct form\n",
      "Number of unique mrns in val cohort: 452\n",
      "Number of target mrns in notes: 175\n",
      "Number of notes from validated mrns: 4642\n",
      "['Progress Notes' 'Consult (Initial)' 'Consult (Follow-Up)']\n",
      "Number of Progress notes from target mrns: 4590\n",
      "in notes: [ 852008 1390547 2050963 1566343 1589928 1261329 1868643 2209904 1420705\n",
      " 1504532 3085711 2460803 2573707 2107646 2714226 3244910 2743967 3324677\n",
      " 2840501 3231751 2903558 2910738 2979285 3134848 2975979 3074909 3259252\n",
      " 3108147 3123549 3418273 2661812 3480721 3497265 3142993 3148181 3151830\n",
      " 3162335 3171632 3199771 3221015 3232866 3249882 3251064 3252015 3260592\n",
      " 2849915 3272590 3370728 3261049 2913559 3105416 2929012 3054505 2933746\n",
      " 2954558 2958683 3276187 2975703 3279359 3290497 3292860 2994570 3001074\n",
      " 3542051 3305006 3307560 3309751 3327545 3331397 3028615 3351851 3351990\n",
      " 3361810 3375948 3385495 3040452 3044387 3415365 3421057 3423189 3426220\n",
      " 3439308 3442639 3447609 3448959 3452129 3453332 3456018 3457262 3470980\n",
      " 3480615 3497403 3503535 3503678 3506201 3507074 3508433 3511774 3515780\n",
      " 3519179 3519848 3521040 3522111 3523447 3524825 3525788 3382796 3317572\n",
      " 3156002 3161158 3163558 3289711 3165977 3535574 3539092 3540920 3543765\n",
      " 3546446 3552528 3553505 3557665 3567685 3580291 3581525 3657187 3191617\n",
      " 3192729 3712308 3712440 3715744 3720166 3727559 3728585 3735514 3736106\n",
      " 3749634 3752745 3755407 3757140 3767186 3793494 3205487 3212148 3228568\n",
      " 3535060 3513634 3256241 3288379 3301890 3372233 3446735 3497326 3345169\n",
      " 3382952 3390635 3437803 3444300 3447668 3486779 3557231 3496132 3503434\n",
      " 3545350 3546090 3555469 3581974 3659799 3667934 3703496 3709726 3713835\n",
      " 3724219 3769590 3772040 3797673]\n",
      "not in notes: [540601, 654028, 656036, 724115, 803715, 816506, 927364, 990013, 991557, 994594, 995741, 1037456, 1070454, 1143622, 1243678, 1403342, 1415504, 1421402, 1430302, 1600901, 1631797, 1656380, 1735432, 1834727, 1893019, 1896755, 1918636, 1919417, 1980198, 1987131, 2147566, 2247359, 2249552, 2262536, 2263614, 2285852, 2297503, 2299167, 2303444, 2321107, 2323950, 2345400, 2367965, 2388169, 2390529, 2424546, 2450905, 2451596, 2455046, 2480287, 2501243, 2503109, 2512720, 2551966, 2552775, 2574798, 2575702, 2593524, 2594631, 2603213, 2609433, 2612713, 2618129, 2623193, 2625779, 2630671, 2637473, 2656523, 2664470, 2669377, 2669482, 2681407, 2688282, 2709091, 2719501, 2723340, 2731502, 2734580, 2753762, 2755561, 2766384, 2766597, 2776756, 2781644, 2783953, 2785400, 2785834, 2816156, 2826015, 2837189, 2843861, 2868922, 2875597, 2876408, 2887246, 2898908, 2901493, 2908665, 2919138, 2928063, 2933360, 2935265, 2940131, 2940960, 2944203, 2948974, 2949542, 2950634, 2954450, 2954930, 2957922, 2963377, 2966304, 2971505, 2973568, 2977050, 2981960, 3018848, 3020230, 3025039, 3026259, 3030364, 3030599, 3033626, 3037165, 3044993, 3045546, 3050435, 3057402, 3058797, 3073499, 3075401, 3102368, 3111711, 3116740, 3120365, 3124524, 3125635, 3130151, 3155132, 3169033, 3179594, 3538715, 1267842, 3508689, 3495811, 3486674, 3459012, 3547729, 3549322, 3491394, 3488621, 3454801, 3454596, 3453307, 2054175, 3461589, 3413871, 3793160, 2507600, 1873071, 3752533, 2630128, 3757628, 3034289, 765010, 770827, 788213, 1173322, 1381378, 1401130, 1627444, 1720174, 1827133, 1877486, 2085028, 2097825, 2124703, 2415350, 2562716, 2577774, 2583627, 2587013, 2596589, 2691234, 2755230, 2860425, 2876217, 2904732, 2928998, 2944057, 2958310, 2964759, 2965261, 2988311, 2994268, 3022684, 3023224, 3107343, 3146164, 3155007, 3178220, 3244907, 3245837, 3252742, 3258201, 3264563, 3288849, 3293120, 3301350, 3316053, 3319861, 3324338, 3324809, 3329485, 3332588, 3333638, 3337954, 3339368, 3341761, 3344282, 3344357, 3348656, 3348843, 3351318, 3353960, 3358561, 3358626, 3359617, 3362902, 3368869, 3369667, 3370866, 3372511, 3375935, 3393004, 3397213, 3399571, 3404599, 3406458, 3409069, 3409090, 3409498, 3412506, 3416409, 3417717, 3418818, 3420288, 3423223, 3424002, 3426469, 3444494, 3448196, 3450138, 3450330, 3453285, 3457054, 3461182, 3465288, 3465461, 3466655, 3472748, 3473006, 3475731, 3476050, 3478403, 3481772, 3482389, 3506990, 3510834, 3512632, 3518389, 3532726, 3534499, 3544750, 3546265, 3546865]\n"
     ]
    }
   ],
   "source": [
    "### load mrns of interest and filter for them\n",
    "file='all_mrns_of_interest.csv'\n",
    "# file='UC_recurrent_mrns.csv'\n",
    "mrn_csv=pd.read_csv(file)\n",
    "\n",
    "# convert mrns with '-' to without\n",
    "try:\n",
    "    mrn_csv.MRN=mrn_csv.MRN.str.replace('-','')\n",
    "    mrn_csv.MRN=mrn_csv.MRN.astype('int64')\n",
    "except:\n",
    "    print('mrns are already in correct form')\n",
    "\n",
    "mrns=mrn_csv.MRN.unique()\n",
    "print('Number of unique mrns in val cohort:',len(mrns))\n",
    "\n",
    "### for 8980\n",
    "# df_8980=mrn_csv[mrn_csv['Source']=='8980']\n",
    "# mrns_8980=df_8980.MRN.unique()\n",
    "# count=0\n",
    "# for mrn in mrns_8980:\n",
    "#     if mrn in data['mrn'].unique():\n",
    "#         count+=1\n",
    "# print ('number of 8980 ptxs in notes:',count)\n",
    "# ### for OPTIMA\n",
    "# df_OPTIMA=mrn_csv[mrn_csv['Source']=='OPTIMA']\n",
    "# mrns_OPTIMA=df_OPTIMA.MRN.unique()\n",
    "# count=0\n",
    "# for mrn in mrns_OPTIMA:\n",
    "#     if mrn in data['mrn'].unique():\n",
    "#         count+=1\n",
    "# print ('number of OPTIMA ptxs in notes:',count)\n",
    "# ### for non-8980\n",
    "# df_non8980=mrn_csv[mrn_csv['Source']=='Non-8980']\n",
    "# mrns_non8980=df_non8980.MRN.unique()\n",
    "# count=0\n",
    "# for mrn in mrns_non8980:\n",
    "#     if mrn in data['mrn'].unique():\n",
    "#         count+=1\n",
    "# print ('number of non-8980 ptxs in notes:',count)\n",
    "\n",
    "\n",
    "#filter data to only include notes from target mrns and only Progress Notes\n",
    "data=data[data['mrn'].isin(mrns)]\n",
    "print('Number of target mrns in notes:',len(data.mrn.unique()))\n",
    "print('Number of notes from validated mrns:',data.shape[0])\n",
    "print (data.ip_note_type.unique())\n",
    "data=data[data.ip_note_type=='Progress Notes']\n",
    "print('Number of Progress notes from target mrns:',data.shape[0])\n",
    "\n",
    "### print mrns that show up in notes and those that do not\n",
    "print ('in notes:',data.mrn.unique())\n",
    "l=[]\n",
    "for mrn in mrns:\n",
    "    if mrn not in data.mrn.unique():\n",
    "        l.append(mrn)\n",
    "print ('not in notes:',l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count     4590\n",
      "unique    3186\n",
      "top           \n",
      "freq       375\n",
      "Name: HPI, dtype: object\n",
      "\n",
      "Number of long notes: 1925\n"
     ]
    }
   ],
   "source": [
    "def carve_out(para,start,end=None):\n",
    "    '''take out a piece of a paragraph using inputs of the string, and then the start and optionally the end'''\n",
    "    toReturn=''\n",
    "    search=re.search(start,para)\n",
    "    if not search:\n",
    "        return toReturn\n",
    "    \n",
    "    # cut off everything to the left of start\n",
    "    toReturn=re.split(start,para,maxsplit=1)[1]\n",
    "    \n",
    "    # cut off everything to the right of end\n",
    "    if end != None:\n",
    "        toReturn=re.split(str(end),toReturn)[0]\n",
    "    \n",
    "    return toReturn\n",
    "\n",
    "def pattern_bank(key):\n",
    "    HPI_pattern=r'HPI|History of Present Illness|HISTORY OF PRESENT ILLNESS|CHIEF COMPLAINT|Chief Complaint'\n",
    "    \n",
    "    Hx_pattern=r'Past Medical History|PAST MEDICAL HISTORY| [Pp][Mm]?[Hh][Xx]|(PAST|Past) 24'\n",
    "    OncHx_pattern=r'([Oo]nc|ONC)(ology|ologic|ological|OLOGY|ONCOLOGICAL|OLOGIC)? [Hh]([Xx]|istory|ISTORY)'\n",
    "    Rx_pattern=r'Current Outpatient Prescriptions|Current Medications'\n",
    "    RoS_pattern=r'Review of Systems|REVIEW OF SYSTEMS|ROS |RoS'\n",
    "    Misc_pattern=r'ALLERGIES|Allergies|ATTENDING|Additional current complaint'\n",
    "    \n",
    "    term_patterns=[Hx_pattern,Rx_pattern,RoS_pattern,Misc_pattern]\n",
    "    ### 7/14/20 removed onc_hx pattern because I actually want it now for extraction\n",
    "    HPI_end=''\n",
    "    for index,pattern in enumerate(term_patterns):\n",
    "        if index==0:\n",
    "            HPI_end=pattern\n",
    "        else:\n",
    "            HPI_end=HPI_end+'|'+pattern\n",
    "    \n",
    "    bank={\n",
    "        'HPI':[HPI_pattern,HPI_end\n",
    "              ]\n",
    "    }\n",
    "    try:\n",
    "        return bank[key]\n",
    "    except:\n",
    "        return ['','']\n",
    "def clean_section(para):\n",
    "    toReturn=para\n",
    "    if toReturn and (':' == toReturn[0] or '.'==toReturn[0]):\n",
    "        toReturn=toReturn[1:]\n",
    "    toReturn=toReturn.strip()\n",
    "    return toReturn\n",
    "    \n",
    "data['HPI']=data.note_narr.apply(carve_out,args=[pattern_bank('HPI')[0],pattern_bank('HPI')[1]])\n",
    "data['HPI']=data.HPI.apply(clean_section)\n",
    "\n",
    "print (data.HPI.describe())\n",
    "\n",
    "\n",
    "def is_big(para):\n",
    "    if len(para)>1000:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "long_ones=data[data.HPI.apply(is_big)]\n",
    "print ('\\nNumber of long notes:',long_ones.shape[0])\n",
    "long_ones.to_csv('dumpyard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All notes with HPI info: 4590->4215\n"
     ]
    }
   ],
   "source": [
    "### get rid of stuff without HPI - it's worthless\n",
    "def hasHPI(HPI):\n",
    "    try:\n",
    "        if type(HPI) is str and len(HPI)>0:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except:\n",
    "        return False\n",
    "before=str(data.shape[0])\n",
    "data=data[data.HPI.apply(hasHPI)]\n",
    "print ('All notes with HPI info: '+before+'->'+str(data.shape[0]))\n",
    "data.to_csv('dumpyard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['date_of_service_dttm', 'mrn', 'note_id', 'ip_note_type',\n",
      "       'author_service', 'note_narr', 'HPI'],\n",
      "      dtype='object')\n",
      "Index(['mrn', 'note_narr', 'HPI', 'date_of_service_dttm'], dtype='object')\n",
      "RangeIndex(start=0, stop=4215, step=1)\n",
      "number notes: 4215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\12488\\python\\lib\\site-packages\\ipykernel_launcher.py:72: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed ptx 1/171\n",
      "Completed ptx 2/171\n",
      "Completed ptx 3/171\n",
      "Completed ptx 4/171\n",
      "Completed ptx 5/171\n",
      "Completed ptx 6/171\n",
      "Completed ptx 7/171\n",
      "Completed ptx 8/171\n",
      "Completed ptx 9/171\n",
      "Completed ptx 10/171\n",
      "Completed ptx 11/171\n",
      "Completed ptx 12/171\n",
      "Completed ptx 13/171\n",
      "Completed ptx 14/171\n",
      "Completed ptx 15/171\n",
      "Completed ptx 16/171\n",
      "Completed ptx 17/171\n",
      "Completed ptx 18/171\n",
      "Completed ptx 19/171\n",
      "Completed ptx 20/171\n",
      "Completed ptx 21/171\n",
      "Completed ptx 22/171\n",
      "Completed ptx 23/171\n",
      "Completed ptx 24/171\n",
      "Completed ptx 25/171\n",
      "Completed ptx 26/171\n",
      "Completed ptx 27/171\n",
      "Completed ptx 28/171\n",
      "Completed ptx 29/171\n",
      "Completed ptx 30/171\n",
      "Completed ptx 31/171\n",
      "Completed ptx 32/171\n",
      "Completed ptx 33/171\n",
      "Completed ptx 34/171\n",
      "Completed ptx 35/171\n",
      "Completed ptx 36/171\n",
      "Completed ptx 37/171\n",
      "Completed ptx 38/171\n",
      "Completed ptx 39/171\n",
      "Completed ptx 40/171\n",
      "Completed ptx 41/171\n",
      "Completed ptx 42/171\n",
      "Completed ptx 43/171\n",
      "Completed ptx 44/171\n",
      "Completed ptx 45/171\n",
      "Completed ptx 46/171\n",
      "Completed ptx 47/171\n",
      "Completed ptx 48/171\n",
      "Completed ptx 49/171\n",
      "Completed ptx 50/171\n",
      "Completed ptx 51/171\n",
      "Completed ptx 52/171\n",
      "Completed ptx 53/171\n",
      "Completed ptx 54/171\n",
      "Completed ptx 55/171\n",
      "Completed ptx 56/171\n",
      "Completed ptx 57/171\n",
      "Completed ptx 58/171\n",
      "Completed ptx 59/171\n",
      "Completed ptx 60/171\n",
      "Completed ptx 61/171\n",
      "Completed ptx 62/171\n",
      "Completed ptx 63/171\n",
      "Completed ptx 64/171\n",
      "Completed ptx 65/171\n",
      "Completed ptx 66/171\n",
      "Completed ptx 67/171\n",
      "Completed ptx 68/171\n",
      "Completed ptx 69/171\n",
      "Completed ptx 70/171\n",
      "Completed ptx 71/171\n",
      "Completed ptx 72/171\n",
      "Completed ptx 73/171\n",
      "Completed ptx 74/171\n",
      "Completed ptx 75/171\n",
      "Completed ptx 76/171\n",
      "Completed ptx 77/171\n",
      "Completed ptx 78/171\n",
      "Completed ptx 79/171\n",
      "Completed ptx 80/171\n",
      "Completed ptx 81/171\n",
      "Completed ptx 82/171\n",
      "Completed ptx 83/171\n",
      "Completed ptx 84/171\n",
      "Completed ptx 85/171\n",
      "Completed ptx 86/171\n",
      "Completed ptx 87/171\n",
      "Completed ptx 88/171\n",
      "Completed ptx 89/171\n",
      "Completed ptx 90/171\n",
      "Completed ptx 91/171\n",
      "Completed ptx 92/171\n",
      "Completed ptx 93/171\n",
      "Completed ptx 94/171\n",
      "Completed ptx 95/171\n",
      "Completed ptx 96/171\n",
      "Completed ptx 97/171\n",
      "Completed ptx 98/171\n",
      "Completed ptx 99/171\n",
      "Completed ptx 100/171\n",
      "Completed ptx 101/171\n",
      "Completed ptx 102/171\n",
      "Completed ptx 103/171\n",
      "Completed ptx 104/171\n",
      "Completed ptx 105/171\n",
      "Completed ptx 106/171\n",
      "Completed ptx 107/171\n",
      "Completed ptx 108/171\n",
      "Completed ptx 109/171\n",
      "Completed ptx 110/171\n",
      "Completed ptx 111/171\n",
      "Completed ptx 112/171\n",
      "Completed ptx 113/171\n",
      "Completed ptx 114/171\n",
      "Completed ptx 115/171\n",
      "Completed ptx 116/171\n",
      "Completed ptx 117/171\n",
      "Completed ptx 118/171\n",
      "Completed ptx 119/171\n",
      "Completed ptx 120/171\n",
      "Completed ptx 121/171\n",
      "Completed ptx 122/171\n",
      "Completed ptx 123/171\n",
      "Completed ptx 124/171\n",
      "Completed ptx 125/171\n",
      "Completed ptx 126/171\n",
      "Completed ptx 127/171\n",
      "Completed ptx 128/171\n",
      "Completed ptx 129/171\n",
      "Completed ptx 130/171\n",
      "Completed ptx 131/171\n",
      "Completed ptx 132/171\n",
      "Completed ptx 133/171\n",
      "Completed ptx 134/171\n",
      "Completed ptx 135/171\n",
      "Completed ptx 136/171\n",
      "Completed ptx 137/171\n",
      "Completed ptx 138/171\n",
      "Completed ptx 139/171\n",
      "Completed ptx 140/171\n",
      "Completed ptx 141/171\n",
      "Completed ptx 142/171\n",
      "Completed ptx 143/171\n",
      "Completed ptx 144/171\n",
      "Completed ptx 145/171\n",
      "Completed ptx 146/171\n",
      "Completed ptx 147/171\n",
      "Completed ptx 148/171\n",
      "Completed ptx 149/171\n",
      "Completed ptx 150/171\n",
      "Completed ptx 151/171\n",
      "Completed ptx 152/171\n",
      "Completed ptx 153/171\n",
      "Completed ptx 154/171\n",
      "Completed ptx 155/171\n",
      "Completed ptx 156/171\n",
      "Completed ptx 157/171\n",
      "Completed ptx 158/171\n",
      "Completed ptx 159/171\n",
      "Completed ptx 160/171\n",
      "Completed ptx 161/171\n",
      "Completed ptx 162/171\n",
      "Completed ptx 163/171\n",
      "Completed ptx 164/171\n",
      "Completed ptx 165/171\n",
      "Completed ptx 166/171\n",
      "Completed ptx 167/171\n",
      "Completed ptx 168/171\n",
      "Completed ptx 169/171\n",
      "Completed ptx 170/171\n",
      "Completed ptx 171/171\n",
      "7415\n"
     ]
    }
   ],
   "source": [
    "### past extraction\n",
    "\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "import sys\n",
    "#!{sys.executable} -m pip install datefinder\n",
    "import dateparser\n",
    "from datetime import datetime\n",
    "\n",
    "def extract_HPI(HPI):\n",
    "    '''takes HPI, returns list of tuples (date,sentence)'''\n",
    "    try:\n",
    "        para=HPI\n",
    "        # start with pre-processing\n",
    "        para=re.sub('\\s{2,}', '. ', para.strip())\n",
    "        para=para.replace('s/p','.')\n",
    "        para=para.replace('year','yr')\n",
    "        para=para.replace(',','!')\n",
    "        para=para.replace('-','--')\n",
    "        para=para.replace('..','.')\n",
    "        #break into sentences and replace the ! because its dumb\n",
    "        sentences=sent_tokenize(para)\n",
    "        sentences=[s.replace('!','.') for s in sentences]\n",
    "        # load spacy stuff\n",
    "        nlp=spacy.load(\"en_core_web_sm\")\n",
    "        doc=nlp(para)\n",
    "        matcher = Matcher(nlp.vocab)\n",
    "        #define Matcher patterns\n",
    "        pattern=[{'ENT_TYPE':'DATE','LIKE_NUM':False},{'ENT_TYPE':'DATE','OP':'+'}]\n",
    "        pattern2=[{\"TEXT\": {\"REGEX\":\"(\\d+/\\d+/\\d+)\"}}]\n",
    "        pattern3=[{\"TEXT\": {\"REGEX\":\"(\\d+/\\d{4})\"}}]\n",
    "        matcher.add('Date Ent Pattern',None,pattern)\n",
    "        matcher.add('Date Pattern',None,pattern2)\n",
    "        matcher.add('Date Pattern',None,pattern3)\n",
    "        # do matching and extraction\n",
    "        toReturn=[]\n",
    "        matches=matcher(doc)\n",
    "        for match_id, start, end in matches:\n",
    "            match=doc[start:end].text\n",
    "            date=''\n",
    "            date=dateparser.parse(match, settings={'PREFER_DAY_OF_MONTH': 'first'})\n",
    "            if date is not None:\n",
    "                if (date.year != 2020):\n",
    "                    #print('\\nMatch found:', doc[start:end].text)\n",
    "                    #print (date)\n",
    "                    for sent in sentences:\n",
    "                            if re.search(match,sent) and date is not None:\n",
    "                                toReturn.append((date,sent))\n",
    "                                #print (sent)\n",
    "        return toReturn\n",
    "    except:\n",
    "        return []\n",
    "def extract_past(df):\n",
    "    '''takes input of df and appends extracted notes to end of it'''\n",
    "    df_start=df\n",
    "    data=[]\n",
    "    mrn=df.iloc[0]['mrn']\n",
    "    for index,row in df_start.iterrows():\n",
    "        HPI=row['HPI']\n",
    "        l=extract_HPI(HPI)\n",
    "        try:\n",
    "            for tup in l:\n",
    "                # want to append: mrn,note_narr/HPI, and date_of_service_dttm\n",
    "                data.append([mrn,tup[1],tup[1],tup[0]])\n",
    "        except:\n",
    "            return df_start\n",
    "    try:\n",
    "        df_to_add=pd.DataFrame(data,columns=['mrn','note_narr','HPI','date_of_service_dttm'])\n",
    "        df_to_add[\"date_of_service_dttm\"] = pd.to_datetime(df_to_add[\"date_of_service_dttm\"])\n",
    "        df_start[\"date_of_service_dttm\"] = pd.to_datetime(df_start[\"date_of_service_dttm\"])\n",
    "        df_total=df_start.append(df_to_add)\n",
    "        df_total=df_total.drop_duplicates(subset=['HPI','date_of_service_dttm'],keep='first')\n",
    "        df_total=df_total.sort_values(by='date_of_service_dttm')\n",
    "        return df_total\n",
    "    except:\n",
    "        return df_start\n",
    "\n",
    "\n",
    "# filter down columns\n",
    "data=pd.read_csv('dumpyard.csv',parse_dates=True)\n",
    "print (data.columns)\n",
    "cols=['mrn','note_narr','HPI','date_of_service_dttm']\n",
    "data=data[cols]\n",
    "print (data.columns)\n",
    "print (data.index)\n",
    "print ('number notes:',data.shape[0])\n",
    "\n",
    "### get unique mrns\n",
    "unique_mrns=data.mrn.unique()\n",
    "#create list of dataframes - each represents an mrn\n",
    "ptx_info=[]\n",
    "for mrn in unique_mrns:\n",
    "    df_sub=data[data.mrn==mrn]\n",
    "    ptx_info.append(df_sub)\n",
    "\n",
    "for index,ptx in enumerate(ptx_info):\n",
    "    ptx_info[index]=extract_past(ptx)\n",
    "    print('Completed ptx '+str(index+1)+'/'+str(len(ptx_info)))\n",
    "    \n",
    "df_rebuilt=''\n",
    "for index,ptx in enumerate(ptx_info):\n",
    "    if index==0:\n",
    "        df_rebuilt=ptx_info[0]\n",
    "    else:\n",
    "        df_rebuilt=df_rebuilt.append(ptx)\n",
    "        \n",
    "# set index back to datetime index\n",
    "data=df_rebuilt.set_index('date_of_service_dttm')\n",
    "print (data.shape[0])\n",
    "data.to_csv('dumpyard.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count     7415\n",
      "unique     144\n",
      "top           \n",
      "freq      4627\n",
      "Name: TNM_stage, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from nltk import download,tokenize\n",
    "\n",
    "def get_first_sentence(para):\n",
    "    '''takes paragraph, returns first sentence'''\n",
    "    try:\n",
    "        sentences=tokenize.sent_tokenize(para)\n",
    "        return sentences[0]\n",
    "    except:\n",
    "        return ''\n",
    "    \n",
    "    \n",
    "def getStage(para):\n",
    "    '''gets TNM stage of tumor from input string'''\n",
    "    pattern=r'[cpyr]?([Tt]\\d\\-)?[Tt][\\dxX](\\w)?( )?([Nn]\\d\\-)?[Nn][\\dxX]?(\\w)?( )?[Mm]?(\\d|[Xx])?'\n",
    "    search=re.search(pattern,para)\n",
    "    stage=''\n",
    "    if search:\n",
    "        stage=search.group()\n",
    "    return stage\n",
    "data['First_Sent']=data.HPI.apply(get_first_sentence)\n",
    "data['TNM_stage']=data.HPI.apply(getStage)\n",
    "data.to_csv('dumpyard.csv')\n",
    "\n",
    "print (data.TNM_stage.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AxesSubplot(0.125,0.125;0.775x0.755)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### add staging\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df=pd.read_csv('dumpyard.csv',parse_dates=True,index_col='date_of_service_dttm')\n",
    "### define functions to get T value, N value, M value\n",
    "def getT(stage):\n",
    "    '''gets the value for T from stage'''\n",
    "    try:\n",
    "        toReturn=stage.upper().split('N')[0]\n",
    "        if toReturn[0] !='T':\n",
    "            toReturn=toReturn[1:]    \n",
    "        toReturn=toReturn[1:].strip()\n",
    "        return toReturn\n",
    "    except:\n",
    "        return np.nan\n",
    "def getN(stage):\n",
    "    '''gets value for N from stage'''\n",
    "    try:\n",
    "        toReturn=stage.upper().split('M')[0]\n",
    "        toReturn=toReturn.split('N')[1].strip()\n",
    "        return toReturn\n",
    "    except:\n",
    "        return np.nan\n",
    "def getM(stage,sent):\n",
    "    '''gets values for M from stage, assumes no mention of M is the same as M0'''\n",
    "    try:\n",
    "        if hasMetsLanguage(sent):\n",
    "            return '1'\n",
    "        elif 'M' not in stage:\n",
    "            return '0'\n",
    "        else:\n",
    "            toReturn=stage.upper().split('M')[1].strip()\n",
    "            return toReturn\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "df['T']=df.TNM_stage.apply(getT)\n",
    "df['N']=df.TNM_stage.apply(getN)\n",
    "\n",
    "Ms=[]\n",
    "for index,row in df.iterrows():\n",
    "    stage=row['TNM_stage']\n",
    "    sent=row['First_Sent']\n",
    "    Ms.append(getM(stage,sent))\n",
    "df['M']=Ms\n",
    "\n",
    "def hasNegativeLanguage(sentence):\n",
    "    '''check if a sentence expresses negative language'''\n",
    "    pattern=r'[Nn]o|[Nn][Ee][Dd]|[Ww]ithout|does not have|[Ss]uspicious|Metastatic Sites NA|concerned about|negative|lack of|(likely|may) represent'\n",
    "    search=re.search(pattern,sentence)\n",
    "    flag=False\n",
    "    if search:\n",
    "        flag=True\n",
    "    return flag\n",
    "def hasMetsLanguage(sentence):\n",
    "    '''check if a sentence indicates a recurrent tumor or metastasis'''\n",
    "    flag=False\n",
    "    search=re.search(r'[Mm]et(s|astatic|astas[ie]s) ',sentence)\n",
    "    if search and not hasNegativeLanguage(sentence):\n",
    "        flag=True\n",
    "    return flag\n",
    "            \n",
    "def get_Cancer_Stage(T,N,M):\n",
    "    '''get stage of disease from TNM inputs, using AJCC 7th criteria'''\n",
    "    stage=''\n",
    "    if str(M)=='1':\n",
    "        stage='IVc'\n",
    "    elif len(str(T))==0 and len(str(N))==0:\n",
    "        stage=''\n",
    "    elif str(T)=='1' and str(N)=='0':\n",
    "        stage='I'\n",
    "    elif str(T)=='2' and str(N)=='0':\n",
    "        stage='II'\n",
    "    elif (str(T)=='3' and str(N)<='1') or (str(T)<='2' and str(N)=='1'):\n",
    "        stage='III'\n",
    "    elif ((str(T)=='4' or str(T)=='4A') and (str(N)<='2C')) or (str(T)>='1' and str(T)<'4' and str(N)>='2' and str(N)<='2C'):\n",
    "        stage='IVa'\n",
    "    elif str(T)=='4B' or (str(N)>'3' and str(N)<'5'):\n",
    "        stage='IVb'\n",
    "    else:\n",
    "        stage=''\n",
    "    return stage\n",
    "\n",
    "cancer_stages=[]\n",
    "for index,row in df.iterrows():\n",
    "    T=row['T']\n",
    "    N=row['N']\n",
    "    M=row['M']\n",
    "    cancer_stages.append(get_Cancer_Stage(T,N,M))\n",
    "df['Cancer_stage']=cancer_stages\n",
    "\n",
    "print (df.Cancer_stage.value_counts().plot(kind='bar'))\n",
    "plt.savefig('Recur_Db_Ptx_by_Stage.png')\n",
    "df=df.sort_values(by=['date_of_service_dttm'])\n",
    "mrns=df.mrn.unique()\n",
    "sub_dfs=[]\n",
    "for mrn in mrns:\n",
    "    sub_df=df[df['mrn']==mrn]\n",
    "    sub_dfs.append(sub_df)\n",
    "df_rebuilt=None\n",
    "for index,sub_df in enumerate(sub_dfs):\n",
    "    if index==0:\n",
    "        df_rebuilt=sub_df\n",
    "    else:\n",
    "        df_rebuilt=df_rebuilt.append(sub_df)\n",
    "df=df_rebuilt\n",
    "df.to_csv('HNDB Progress Notes Processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\12488\\python\\lib\\site-packages\\ipykernel_launcher.py:53: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "from nltk import download,tokenize\n",
    "\n",
    "df=pd.read_csv('HNDB Progress Notes Processed.csv', parse_dates=True,index_col='date_of_service_dttm')\n",
    "def find_the_diff(para1,para2):\n",
    "    '''returns all the different sentences between two paragraphs, organized as a single string'''\n",
    "    sentences1=tokenize.sent_tokenize(para1)\n",
    "    sentences2=tokenize.sent_tokenize(para2)\n",
    "    new_sentences=[]\n",
    "    for sentence in sentences2:\n",
    "        if sentence not in sentences1:\n",
    "            new_sentences.append(sentence)\n",
    "    toReturn=''\n",
    "    for sentence in new_sentences:\n",
    "        toReturn=toReturn+sentence+' '\n",
    "    return toReturn\n",
    "\n",
    "def list_of_diffs(df,col_name1,col_name2):\n",
    "    '''returns a list of the differences in a dataframe between two columns'''\n",
    "    toReturn=[]\n",
    "    for index,row in df.iterrows():\n",
    "        if index==0:\n",
    "            toReturn.append('')\n",
    "        else:\n",
    "            para1=row[col_name1]\n",
    "            para2=row[col_name2]\n",
    "            diffs=find_the_diff(para1,para2)\n",
    "            toReturn.append(diffs)\n",
    "    return toReturn\n",
    "def hide_old_sentences(df):\n",
    "    sentences=[]\n",
    "    new_HPIs=[]\n",
    "    for index,row in df.iterrows():\n",
    "        sents=tokenize.sent_tokenize(row['HPI'])\n",
    "        new_HPI=''\n",
    "        for s in sents:\n",
    "            if s not in sentences:\n",
    "                sentences.append(s)\n",
    "                new_HPI=new_HPI+s+' '\n",
    "        new_HPIs.append(new_HPI)\n",
    "    df['filtered_HPI']=new_HPIs\n",
    "    return df\n",
    "\n",
    "\n",
    "### get unique mrns\n",
    "unique_mrns=df.mrn.unique()\n",
    "#create list of dataframes - each represents an mrn\n",
    "ptx_info=[]\n",
    "for mrn in unique_mrns:\n",
    "    df_sub=df[df.mrn==mrn]\n",
    "    ptx_info.append(df_sub)\n",
    "\n",
    "for index,ptx in enumerate(ptx_info):\n",
    "    ptx_info[index]['last_HPI']=ptx_info[index]['HPI'].shift(1)\n",
    "    ptx_info[index] = ptx_info[index].replace(np.nan, '', regex=True)\n",
    "    ptx_info[index]['changes_in_HPI']=list_of_diffs(ptx_info[index],'last_HPI','HPI')\n",
    "    ptx_info[index]=hide_old_sentences(ptx_info[index])\n",
    "    \n",
    "df_rebuilt=''\n",
    "for index,ptx in enumerate(ptx_info):\n",
    "    if index==0:\n",
    "        df_rebuilt=ptx_info[0]\n",
    "    else:\n",
    "        df_rebuilt=df_rebuilt.append(ptx)\n",
    "df=df_rebuilt\n",
    "df.index = df.index.normalize()\n",
    "df.to_csv('HNDB Progress Notes Processed.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
